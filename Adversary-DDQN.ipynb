{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "becc4ed8",
   "metadata": {},
   "source": [
    "# Adversary Double Deep Q-Networks (DDQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bf1a3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthomasvroom\u001b[0m (\u001b[33mthomasvroom-maastricht-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cores available: 12\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import wandb\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange\n",
    "import multiprocessing\n",
    "import gymnasium as gym\n",
    "from src.env import CustomLunarLander, AdversarialLunarLander\n",
    "from src.models import DDQN_Agent, ReplayBuffer\n",
    "from src import util\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "wandb.login()\n",
    "np.seterr(all='raise'); # raise exceptions on errors\n",
    "print(f\"Number of cores available: {multiprocessing.cpu_count()}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\") # device for pytorch\n",
    "gym.register(id=\"CustomLunarLander-v0\", entry_point=CustomLunarLander)\n",
    "gym.register(id=\"AdversarialLunarLander-v0\", entry_point=AdversarialLunarLander)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17cc0c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, run_name):\n",
    "    run = wandb.init(\n",
    "        project=\"RL\",\n",
    "        entity=\"thomasvroom-maastricht-university\",\n",
    "        config=config,\n",
    "        name=run_name\n",
    "    )\n",
    "\n",
    "    # seeding\n",
    "    random.seed(config[\"random_seed\"])\n",
    "    np.random.seed(config[\"random_seed\"])\n",
    "    torch.manual_seed(config[\"random_seed\"])\n",
    "    torch.backends.cudnn.deterministic = config[\"deterministic\"]\n",
    "\n",
    "    # create environment (only 1, since bottleneck isn't experience gathering)\n",
    "    env = gym.make(\n",
    "        id=\"AdversarialLunarLander-v0\",\n",
    "        gravity=config[\"gravity\"],\n",
    "        wind_power=config[\"wind_power\"],\n",
    "        turbulence_power=config[\"turbulence_power\"],\n",
    "        max_episode_steps=config[\"max_env_steps\"]\n",
    "    )\n",
    "\n",
    "    # create both protagonist and adversary\n",
    "    protagonist = DDQN_Agent(env.observation_space.shape[0], env.action_space[0].n).to(device)\n",
    "    adversary = DDQN_Agent(env.observation_space.shape[0], env.action_space[1].n).to(device) # see AdversarialLunarLander.py\n",
    "    optimizer_protagonist = torch.optim.AdamW(protagonist.parameters(), weight_decay=config[\"weight_decay\"], lr=config[\"learning_rate\"])\n",
    "    optimizer_adversary = torch.optim.AdamW(adversary.parameters(), weight_decay=config[\"weight_decay\"], lr=config[\"learning_rate\"])\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    # create target network and replay buffer for both players\n",
    "    target_network_protagonist = type(protagonist)(env.observation_space.shape[0], env.action_space[0].n).to(device)\n",
    "    target_network_protagonist.load_state_dict(protagonist.state_dict())\n",
    "    replay_buffer_protagonist = ReplayBuffer(config[\"buffer_size\"])\n",
    "    target_network_adversary = type(adversary)(env.observation_space.shape[0], env.action_space[1].n).to(device)\n",
    "    target_network_adversary.load_state_dict(adversary.state_dict())\n",
    "    replay_buffer_adversary = ReplayBuffer(config[\"buffer_size\"])\n",
    "\n",
    "    epsilon_protagonist = 1\n",
    "    epsilon_adversary = 1\n",
    "    global_learning_steps = 0\n",
    "    protagonist_learning_steps = 0\n",
    "    adversary_learning_steps = 0\n",
    "\n",
    "    def train_episode(training_protagonist, epsilon, global_steps, local_steps, episode):\n",
    "        # reset environment\n",
    "        state, _ = env.reset(seed=None if episode > 0 else config[\"random_seed\"])\n",
    "        done = False\n",
    "        truncated = False\n",
    "        total_reward = 0\n",
    "\n",
    "        # run environment until done\n",
    "        while not (done or truncated):\n",
    "            # epsilon-greedy action selection (protagonist)\n",
    "            if not training_protagonist or np.random.random() > epsilon:\n",
    "                with torch.no_grad():\n",
    "                    observation = torch.tensor(state, dtype=torch.float).to(device)\n",
    "                    protagonist_action = protagonist.get_action(observation).item()\n",
    "            else:\n",
    "                protagonist_action = env.action_space[0].sample()\n",
    "            # epsilon-greedy action selection (adversary)\n",
    "            if training_protagonist or np.random.random() > epsilon:\n",
    "                with torch.no_grad():\n",
    "                    observation = torch.tensor(state, dtype=torch.float).to(device)\n",
    "                    adversary_action = adversary.get_action(observation).item()\n",
    "            else:\n",
    "                adversary_action = env.action_space[1].sample()\n",
    "\n",
    "            # execute action\n",
    "            new_state, reward, done, truncated, _ = env.step([protagonist_action, adversary_action])\n",
    "\n",
    "            # add sample to replay buffer\n",
    "            if training_protagonist:\n",
    "                replay_buffer_protagonist.add_new_sample(state, protagonist_action, reward, new_state, done)\n",
    "            else: # zero-sum game, so reward is inversed\n",
    "                replay_buffer_adversary.add_new_sample(state, adversary_action, -reward, new_state, done)\n",
    "\n",
    "            state = new_state\n",
    "            total_reward += reward\n",
    "\n",
    "            # only update one player at a time\n",
    "            agent = protagonist if training_protagonist else adversary\n",
    "            replay_buffer = replay_buffer_protagonist if training_protagonist else replay_buffer_adversary\n",
    "            target_network = target_network_protagonist if training_protagonist else target_network_adversary\n",
    "            optimizer = optimizer_protagonist if training_protagonist else optimizer_adversary\n",
    "\n",
    "            # only update weights if there are enough samples\n",
    "            if len(replay_buffer) > config[\"batch_size\"]:\n",
    "                # replace target network\n",
    "                if local_steps % config[\"target_replace_steps\"] == 0:\n",
    "                    target_network.load_state_dict(agent.state_dict())\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # sample from replay buffer\n",
    "                samples = replay_buffer.get_pytorch_training_samples(device, config[\"batch_size\"])\n",
    "                states, actions, rewards, new_states, was_terminals = samples\n",
    "                indices = np.arange(config[\"batch_size\"])\n",
    "\n",
    "                # get the predicted q-values\n",
    "                q_pred = agent.forward(states)[indices, actions]\n",
    "\n",
    "                # get the estimated next q-values\n",
    "                q_next = target_network.forward(new_states).max(dim=1)[0]\n",
    "                q_next[was_terminals] = 0.0\n",
    "\n",
    "                # target values\n",
    "                q_label = rewards + config[\"gamma\"] * q_next\n",
    "\n",
    "                # calculate and backpropegate loss\n",
    "                loss = loss_fn(q_label, q_pred).to(device)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # decay epsilon and record data\n",
    "                epsilon = max(epsilon - config[\"epsilon_decay\"], config[\"epsilon_min\"])\n",
    "                if training_protagonist:\n",
    "                    run.log({\"loss (protagonist)\": loss, \"epsilon (protagonist)\": epsilon}, global_steps)\n",
    "                else:\n",
    "                    run.log({\"loss (adversary)\": loss, \"epsilon (adversary)\": epsilon}, global_steps)\n",
    "\n",
    "                local_steps += 1\n",
    "                global_steps += 1\n",
    "\n",
    "        run.log({\"total_reward\": total_reward}, max(global_steps, episode))\n",
    "        return epsilon, global_steps, local_steps\n",
    "\n",
    "    # let protagonist and adversary take turns\n",
    "    for episode in trange(config[\"training_cycles\"]):\n",
    "        for _ in trange(config[\"protagonist_episodes\"]):\n",
    "            epsilon_protagonist, global_learning_steps, protagonist_learning_steps = train_episode(\n",
    "                True, epsilon_protagonist, global_learning_steps, protagonist_learning_steps, episode\n",
    "            )\n",
    "        for _ in trange(config[\"adversary_episodes\"]):\n",
    "            epsilon_adversary, global_learning_steps, adversary_learning_steps = train_episode(\n",
    "                False, epsilon_adversary, global_learning_steps, adversary_learning_steps, episode\n",
    "            )\n",
    "\n",
    "    env.close()\n",
    "    run.finish(0)\n",
    "    torch.save(protagonist.state_dict(), f\"models/protagonist-{run_name}\")\n",
    "    torch.save(adversary.state_dict(), f\"models/adversary-{run_name}\")\n",
    "\n",
    "config = { # see: https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
    "    \"gravity\": -10.0,\n",
    "    \"wind_power\": 10.0,\n",
    "    \"turbulence_power\": 1.0,\n",
    "\n",
    "    \"random_seed\": 123,\n",
    "    \"deterministic\": True, # toggles torch.backends.cudnn.deterministic\n",
    "    \"training_cycles\": 4, # how often the protagonist and adversary switch position\n",
    "    \"protagonist_episodes\": 600,\n",
    "    \"adversary_episodes\": 200,\n",
    "    \"buffer_size\": 100_000, # size of the replay buffer\n",
    "    \"batch_size\": 64,\n",
    "    \"target_replace_steps\": 500, # after how many steps the target network gets replaced\n",
    "    \"max_env_steps\": 1000, # number of steps before truncation\n",
    "\n",
    "    \"gamma\": 0.99,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"epsilon_min\": 0.01,\n",
    "    \"epsilon_decay\": 5e-6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24603b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for wandb.init()..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Thoma\\OneDrive\\Documenten\\University\\Year 4\\Period 5\\Reinforcement Learning\\Assignment\\wandb\\run-20250524_151458-pme38azo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thomasvroom-maastricht-university/RL/runs/pme38azo' target=\"_blank\">Adversary-DDQN-1748092498.3286374</a></strong> to <a href='https://wandb.ai/thomasvroom-maastricht-university/RL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thomasvroom-maastricht-university/RL' target=\"_blank\">https://wandb.ai/thomasvroom-maastricht-university/RL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thomasvroom-maastricht-university/RL/runs/pme38azo' target=\"_blank\">https://wandb.ai/thomasvroom-maastricht-university/RL/runs/pme38azo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8f1e219218040929e0a3d7aeba4bfbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "320299da441b461590a2dac676c33afe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "952a2b6914f6469e92902b0e1279d3a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d46ac18e20a4e2db0ed2abc722c2ca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82740067d76746669c71f302cd7f7061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc3545a19c0d4547aba3dd66a92c9d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5427c0ba0894131a17dc18c5823363c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224284f57b7e431bb74ac114b2700bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4694cca755fa456ebe8f6501faf51522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epsilon (adversary)</td><td>████▇▇▇▇▆▆▆▄▄▄▄▃▃▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epsilon (protagonist)</td><td>█▆▅▅▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss (adversary)</td><td>▁▁▁▁▁▄▁█▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss (protagonist)</td><td>▅▃▃▂▂█▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▅▃▁▁▁▁▁▂▁▁▁▁▃▁▁▁▂▁▁</td></tr><tr><td>total_reward</td><td>▁▁▁▁▁▁▁▃▁▂▄▃▅▄▁▅▅▅▅▅▅▅▅▅▄▇▅▇▆▇▆█▆▇█████▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epsilon (adversary)</td><td>0.01</td></tr><tr><td>epsilon (protagonist)</td><td>0.01</td></tr><tr><td>loss (adversary)</td><td>2.01521</td></tr><tr><td>loss (protagonist)</td><td>2.52194</td></tr><tr><td>total_reward</td><td>-157.65986</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Adversary-DDQN-1748092498.3286374</strong> at: <a href='https://wandb.ai/thomasvroom-maastricht-university/RL/runs/pme38azo' target=\"_blank\">https://wandb.ai/thomasvroom-maastricht-university/RL/runs/pme38azo</a><br> View project at: <a href='https://wandb.ai/thomasvroom-maastricht-university/RL' target=\"_blank\">https://wandb.ai/thomasvroom-maastricht-university/RL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250524_151458-pme38azo\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_name = f\"Adversary-DDQN-{time.time()}\"\n",
    "train(config, run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd4aea57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected a total reward of: -713.5486136769899\n"
     ]
    }
   ],
   "source": [
    "# run_name = \"Adversary-DDQN-1748037372.4373443\"\n",
    "\n",
    "# load agent\n",
    "protagonist = DDQN_Agent(8, 4).to(device)\n",
    "protagonist.load_state_dict(torch.load(f\"models/protagonist-{run_name}\"))\n",
    "adversary = DDQN_Agent(8, 4).to(device)\n",
    "adversary.load_state_dict(torch.load(f\"models/adversary-{run_name}\"))\n",
    "video_name = None\n",
    "max_time = 30\n",
    "\n",
    "env = gym.make(\n",
    "    \"AdversarialLunarLander-v0\",\n",
    "    gravity=config[\"gravity\"],\n",
    "    wind_power=config[\"wind_power\"],\n",
    "    turbulence_power=config[\"turbulence_power\"],\n",
    "    render_mode=\"rgb_array\" if video_name else \"human\"\n",
    ")\n",
    "if video_name: \n",
    "    env = gym.wrappers.RecordVideo(env, f\"videos/{video_name}\")\n",
    "\n",
    "# reset environment\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "target_time = time.time() + max_time\n",
    "reward_sum = 0\n",
    "while not (done or time.time() > target_time):\n",
    "    # convert observation to tensor\n",
    "    obs = torch.tensor(obs, dtype=torch.float32).to(device)\n",
    "\n",
    "    # sample action from agents\n",
    "    with torch.no_grad():\n",
    "        protagonist_action = protagonist.get_action(obs).item()\n",
    "        adversary_action = adversary.get_action(obs).item()\n",
    "\n",
    "    # execute action\n",
    "    obs, reward, terminated, truncated, __ = env.step([protagonist_action, adversary_action])\n",
    "    done = terminated or truncated\n",
    "    reward_sum += reward\n",
    "    env.render()\n",
    "\n",
    "# close ui\n",
    "env.close()\n",
    "print(f\"Collected a total reward of: {reward_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3927080b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected a total reward of: 272.92410187049927\n"
     ]
    }
   ],
   "source": [
    "# run_name = \"Adversary-DDQN-1748037372.4373443\"\n",
    "\n",
    "# load agent\n",
    "agent = DDQN_Agent(8, 4).to(device)\n",
    "agent.load_state_dict(torch.load(f\"models/protagonist-{run_name}\"))\n",
    "\n",
    "util.visualize_episode(\n",
    "    env_id=\"CustomLunarLander-v0\",\n",
    "    gravity=config[\"gravity\"],\n",
    "    enable_wind=False,\n",
    "    wind_power=config[\"wind_power\"],\n",
    "    turbulence_power=config[\"turbulence_power\"],\n",
    "    agent=agent,\n",
    "    device=device,\n",
    "    max_time=30,\n",
    "    video_name=None\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
