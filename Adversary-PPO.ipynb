{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bffba79b",
   "metadata": {},
   "source": [
    "# Adversary Proximal Policy Optimization (PPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9f55592",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthomasvroom\u001b[0m (\u001b[33mthomasvroom-maastricht-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cores available: 12\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import wandb\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange\n",
    "import multiprocessing\n",
    "import gymnasium as gym\n",
    "\n",
    "from src.env import CustomLunarLander, AdversarialLunarLander\n",
    "from src.models import PPO_Agent\n",
    "from src import util\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "wandb.login()\n",
    "np.seterr(all='raise'); # raise exceptions on errors\n",
    "print(f\"Number of cores available: {multiprocessing.cpu_count()}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\") # device for pytorch\n",
    "gym.register(id=\"CustomLunarLander-v0\", entry_point=CustomLunarLander)\n",
    "gym.register(id=\"AdversarialLunarLander-v0\", entry_point=AdversarialLunarLander)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f7974b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, run_name):\n",
    "    run = wandb.init(\n",
    "        project=\"RL\",\n",
    "        entity=\"thomasvroom-maastricht-university\",\n",
    "        config=config,\n",
    "        name=run_name\n",
    "    )\n",
    "\n",
    "    # seeding\n",
    "    random.seed(config[\"random_seed\"])\n",
    "    np.random.seed(config[\"random_seed\"])\n",
    "    torch.manual_seed(config[\"random_seed\"])\n",
    "    torch.backends.cudnn.deterministic = config[\"deterministic\"]\n",
    "\n",
    "    # create environments\n",
    "    envs = gym.vector.SyncVectorEnv([\n",
    "        util.make_adv_env(\n",
    "            env_id=\"AdversarialLunarLander-v0\",\n",
    "            gravity=config[\"gravity\"],\n",
    "            wind_power=config[\"wind_power\"],\n",
    "            turbulence_power=config[\"turbulence_power\"],\n",
    "            max_episode_steps=config[\"max_env_steps\"]\n",
    "        ) for _ in range(config[\"n_envs\"])],\n",
    "    )\n",
    "\n",
    "    # create protagonist and adversary\n",
    "    protagonist = PPO_Agent(np.array(envs.single_observation_space.shape).prod(), envs.single_action_space[0].n).to(device)\n",
    "    adversary = PPO_Agent(np.array(envs.single_observation_space.shape).prod(), envs.single_action_space[1].n).to(device)\n",
    "    optimizer_protagonist = torch.optim.Adam(protagonist.parameters(), lr=config[\"learning_rate\"], eps=1e-5)\n",
    "    optimizer_adversary = torch.optim.Adam(adversary.parameters(), lr=config[\"learning_rate\"], eps=1e-5)\n",
    "\n",
    "    # more hyperparameters determined at runtime\n",
    "    batch_size = int(config[\"n_envs\"] * config[\"steps_per_batch\"])\n",
    "    minibatch_size = int(batch_size // config[\"num_minibatches\"])\n",
    "    num_iterations_protagonist = int(config[\"protagonist_steps\"] // batch_size)\n",
    "    num_iterations_adversary = int(config[\"adversary_steps\"] // batch_size)\n",
    "\n",
    "    # storage setup\n",
    "    obs = torch.zeros((config[\"steps_per_batch\"], config[\"n_envs\"]) + envs.single_observation_space.shape).to(device)\n",
    "    actions_protagonist = torch.zeros((config[\"steps_per_batch\"], config[\"n_envs\"]) + envs.single_action_space[0].shape).to(device)\n",
    "    actions_adversary = torch.zeros((config[\"steps_per_batch\"], config[\"n_envs\"]) + envs.single_action_space[1].shape).to(device)\n",
    "    logprobs = torch.zeros((config[\"steps_per_batch\"], config[\"n_envs\"])).to(device)\n",
    "    rewards = torch.zeros((config[\"steps_per_batch\"], config[\"n_envs\"])).to(device)\n",
    "    dones = torch.zeros((config[\"steps_per_batch\"], config[\"n_envs\"])).to(device)\n",
    "    values = torch.zeros((config[\"steps_per_batch\"], config[\"n_envs\"])).to(device)\n",
    "\n",
    "    global_step = 0\n",
    "    protagonist_step = 0\n",
    "    adversary_step = 0\n",
    "\n",
    "    # start the environment\n",
    "    start_time = time.time()\n",
    "    next_obs, _ = envs.reset(seed=config[\"random_seed\"])\n",
    "    next_obs = torch.Tensor(next_obs).to(device)\n",
    "    next_done = torch.zeros(config[\"n_envs\"]).to(device)\n",
    "\n",
    "    def train_episode(training_protagonist, global_step, local_step, next_obs, next_done):\n",
    "        # annealing the learning rate\n",
    "        if config[\"anneal_lr\"]:\n",
    "            if training_protagonist:\n",
    "                frac = 1.0 - (local_step - 1.0) / (num_iterations_protagonist * config[\"training_cycles\"])\n",
    "                lrnow = frac * config[\"learning_rate\"]\n",
    "                optimizer_protagonist.param_groups[0][\"lr\"] = lrnow\n",
    "            else:\n",
    "                frac = 1.0 - (local_step - 1.0) / (num_iterations_adversary * config[\"training_cycles\"])\n",
    "                lrnow = frac * config[\"learning_rate\"]\n",
    "                optimizer_adversary.param_groups[0][\"lr\"] = lrnow\n",
    "        local_step += 1\n",
    "\n",
    "        for step in range(0, config[\"steps_per_batch\"]):\n",
    "            global_step += config[\"n_envs\"]\n",
    "            obs[step] = next_obs\n",
    "            dones[step] = next_done\n",
    "\n",
    "            # action logic\n",
    "            with torch.no_grad():\n",
    "                action, logprob, _, value = (protagonist if training_protagonist else adversary).get_action_and_value(next_obs)\n",
    "                values[step] = value.flatten()\n",
    "            (actions_protagonist if training_protagonist else actions_adversary)[step] = action\n",
    "            logprobs[step] = logprob\n",
    "\n",
    "            other_action = (adversary if training_protagonist else protagonist).get_action_and_value(next_obs)[0]\n",
    "            input = np.vstack([action.cpu().numpy(), other_action.cpu().numpy()]).T\n",
    "\n",
    "            # execute the action and log data\n",
    "            next_obs, reward, terminations, truncations, infos = envs.step(input)\n",
    "            next_done = np.logical_or(terminations, truncations)\n",
    "            rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)\n",
    "\n",
    "            if infos and \"episode\" in infos:\n",
    "                indices = np.where(infos[\"_episode\"])\n",
    "                run.log({\n",
    "                    \"charts/episodic_return\": infos[\"episode\"][\"r\"][indices].mean(),\n",
    "                    \"charts/episodic_length\": infos[\"episode\"][\"l\"][indices].mean()\n",
    "                }, global_step)\n",
    "\n",
    "        # bootstrap value if not done\n",
    "        with torch.no_grad():\n",
    "            next_value = (protagonist if training_protagonist else adversary).get_value(next_obs).reshape(1, -1)\n",
    "            advantages = torch.zeros_like(rewards).to(device)\n",
    "            lastgaelam = 0\n",
    "            for t in reversed(range(config[\"steps_per_batch\"])):\n",
    "                if t == config[\"steps_per_batch\"] - 1:\n",
    "                    nextnonterminal = 1.0 - next_done\n",
    "                    nextvalues = next_value\n",
    "                else:\n",
    "                    nextnonterminal = 1.0 - dones[t + 1]\n",
    "                    nextvalues = values[t + 1]\n",
    "                delta = rewards[t] + config[\"gamma\"] * nextvalues * nextnonterminal - values[t]\n",
    "                advantages[t] = lastgaelam = delta + config[\"gamma\"] * config[\"gae_lambda\"] * nextnonterminal * lastgaelam\n",
    "            returns = advantages + values\n",
    "\n",
    "        # flatten the batch\n",
    "        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "        b_logprobs = logprobs.reshape(-1)\n",
    "        if training_protagonist:\n",
    "            b_actions = actions_protagonist.reshape((-1,) + envs.single_action_space[0].shape)\n",
    "        else:\n",
    "            b_actions = actions_adversary.reshape((-1,) + envs.single_action_space[1].shape)\n",
    "        b_advantages = advantages.reshape(-1)\n",
    "        b_returns = returns.reshape(-1)\n",
    "        b_values = values.reshape(-1)\n",
    "\n",
    "        # optimizing the policy and value network\n",
    "        b_inds = np.arange(batch_size)\n",
    "        clipfracs = []\n",
    "        for epoch in range(config[\"policy_epochs\"]):\n",
    "            np.random.shuffle(b_inds)\n",
    "            for start in range(0, batch_size, minibatch_size):\n",
    "                end = start + minibatch_size\n",
    "                mb_inds = b_inds[start:end]\n",
    "\n",
    "                if training_protagonist:\n",
    "                    _, newlogprob, entropy, newvalue = protagonist.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])\n",
    "                else:\n",
    "                    _, newlogprob, entropy, newvalue = adversary.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])\n",
    "                logratio = newlogprob - b_logprobs[mb_inds]\n",
    "                ratio = logratio.exp()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                    old_approx_kl = (-logratio).mean()\n",
    "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                    clipfracs += [((ratio - 1.0).abs() > config[\"clip_coef\"]).float().mean().item()]\n",
    "\n",
    "                mb_advantages = b_advantages[mb_inds]\n",
    "                if config[\"norm_adv\"]:\n",
    "                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "                # policy loss\n",
    "                pg_loss1 = -mb_advantages * ratio\n",
    "                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - config[\"clip_coef\"], 1 + config[\"clip_coef\"])\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                # value loss\n",
    "                newvalue = newvalue.view(-1)\n",
    "                if config[\"clip_vloss\"]:\n",
    "                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                    v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                        newvalue - b_values[mb_inds],\n",
    "                        -config[\"clip_coef\"],\n",
    "                        config[\"clip_coef\"],\n",
    "                    )\n",
    "                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                    v_loss = 0.5 * v_loss_max.mean()\n",
    "                else:\n",
    "                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "                entropy_loss = entropy.mean()\n",
    "                loss = pg_loss - config[\"ent_coef\"] * entropy_loss + v_loss * config[\"vf_coef\"]\n",
    "\n",
    "                (optimizer_protagonist if training_protagonist else optimizer_adversary).zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_((protagonist if training_protagonist else adversary).parameters(), config[\"max_grad_norm\"])\n",
    "                (optimizer_protagonist if training_protagonist else optimizer_adversary).step()\n",
    "\n",
    "            if config[\"target_kl\"] is not None and approx_kl > config[\"target_kl\"]:\n",
    "                break\n",
    "\n",
    "        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "        # record data\n",
    "        run.log({\n",
    "            \"charts/learning_rate\": (optimizer_protagonist if training_protagonist else optimizer_adversary).param_groups[0][\"lr\"],\n",
    "            \"losses/value_loss\": v_loss.item(),\n",
    "            \"losses/policy_loss\": pg_loss.item(),\n",
    "            \"losses/entropy\": entropy_loss.item(),\n",
    "            \"losses/old_approx_kl\": old_approx_kl.item(),\n",
    "            \"losses/approx_kl\": approx_kl.item(),\n",
    "            \"losses/clipfrac\": np.mean(clipfracs),\n",
    "            \"charts/SPS\": int(global_step / (time.time() - start_time))\n",
    "        }, global_step)\n",
    "        if not np.isnan(explained_var):\n",
    "            run.log({\"losses/explained_variance\": explained_var}, global_step)\n",
    "        return global_step, local_step, next_obs, next_done\n",
    "\n",
    "    # let protagonist and adversary take turns\n",
    "    for episode in trange(config[\"training_cycles\"]):\n",
    "        for _ in trange(num_iterations_protagonist):\n",
    "            global_step, protagonist_step, next_obs, next_done = train_episode(True, global_step, protagonist_step, next_obs, next_done)\n",
    "        for _ in trange(num_iterations_adversary):\n",
    "            global_step, adversary_step, next_obs, next_done = train_episode(False, global_step, adversary_step, next_obs, next_done)\n",
    "\n",
    "    envs.close()\n",
    "    run.finish(0)\n",
    "    torch.save(protagonist.state_dict(), f\"models/protagonist-{run_name}\")\n",
    "    torch.save(adversary.state_dict(), f\"models/adversary-{run_name}\")\n",
    "\n",
    "config = { # see: https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
    "    \"gravity\": -10.0,\n",
    "    \"wind_power\": 10.0,\n",
    "    \"turbulence_power\": 1.0,\n",
    "\n",
    "    \"random_seed\": 123,\n",
    "    \"deterministic\": True, # toggles torch.backends.cudnn.deterministic\n",
    "    \"n_envs\": 8,\n",
    "    \"training_cycles\": 8, # how often the protagonist and adversary switch position\n",
    "    \"protagonist_steps\": 400_000,\n",
    "    \"adversary_steps\": 100_000,\n",
    "    \"steps_per_batch\": 2048, # number of steps to run in each env per policy rollout\n",
    "    \"num_minibatches\": 16,\n",
    "    \"policy_epochs\": 8, # number of epochs to update the policy\n",
    "    \"max_env_steps\": 1000, # number of steps before truncation\n",
    "\n",
    "    \"gamma\": 0.99,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"anneal_lr\": True, # toggles lr decay\n",
    "    \"gae_lambda\": 0.95, # lambda for the general advantage estimation\n",
    "    \"clip_coef\": 0.2, # surrogate clipping coefficient\n",
    "    \"norm_adv\": True, # toggles advantage normalization\n",
    "    \"clip_vloss\": False, # toggles use of clipped loss for the value function\n",
    "    \"ent_coef\": 0.01, # entropy coefficient\n",
    "    \"vf_coef\": 0.5, # value function coefficient\n",
    "    \"max_grad_norm\": 0.5, # maximum norm for gradient clipping\n",
    "    \"target_kl\": 0.01 # the target KL divergence threshold\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87a04cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Thoma\\OneDrive\\Documenten\\University\\Year 4\\Period 5\\Reinforcement Learning\\Assignment\\wandb\\run-20250525_180257-bkglldwi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thomasvroom-maastricht-university/RL/runs/bkglldwi' target=\"_blank\">Adversary-PPO-1748188977.5459409</a></strong> to <a href='https://wandb.ai/thomasvroom-maastricht-university/RL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thomasvroom-maastricht-university/RL' target=\"_blank\">https://wandb.ai/thomasvroom-maastricht-university/RL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thomasvroom-maastricht-university/RL/runs/bkglldwi' target=\"_blank\">https://wandb.ai/thomasvroom-maastricht-university/RL/runs/bkglldwi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b817f585f24644bff9396e2e768a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cbf2b4d347847d393279e5397b20f77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f81f6c0f1984d0aacd2b2ba66b9bb2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1550627e0f6c4b759f968669ff3f47be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de75478b8b74626aeaeb9a03dbea72e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20a33a7f1f874b4e8b0cd6e827e85059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "748233fc3ed94a909cb7e9468845f0de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e2771895044176bc2b586167bb0a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ce6fcfadfe4bd9955c3a0580a5baba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e745f9a931084c41b3c4bd338e312b36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b4adaac64674493bbb99ede41fd05c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7d5256954db4148a1018bbcf67ea765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8962c35f0f4a37a9515f6ab9b64b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f841775dae4396ba6affe0a1462a0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5afa2a73bf0645d7afc1de13a5b18c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "066f51b782e84479850523067b916db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab681bd4aeb842b88484986b1ac6eff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>charts/SPS</td><td>▁▆▆▆▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███████████</td></tr><tr><td>charts/episodic_length</td><td>▁▁▁▁▂▂▂▃▁▁▁▁▁█▅▁▁▁▁▁▁█▂▂▅██▂█▅█▇▄▆██▂██▅</td></tr><tr><td>charts/episodic_return</td><td>▄▅▁▃▂▃▂▅▃▃▄▇▃▃▂▅▇▃▂▂▄▆▆▄▅▄▇█▇▇▆▇▆▇▇▇▆▆▆▇</td></tr><tr><td>charts/learning_rate</td><td>█████▇▇▇▇▇▆▇▇▆▆▅▅▅▅▅▅▅▅▄▄▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>losses/approx_kl</td><td>▆▆▇█▇▄▅▆▆▄▃▄▅▇▅▃▃▅▄▃▅▃▃▃▄▄▄▄▃▃▄▂▄▂▆▅▂▄▃▁</td></tr><tr><td>losses/clipfrac</td><td>▃▅▆▇▃▄▄██▂▄█▃▂▃▆▃▃▂▅▂▂▂▅▂▁▂▃▂▂▆▃▄▄▁▁▃▁▁▁</td></tr><tr><td>losses/entropy</td><td>█▆▆▅▃▅▄▄▃▄▃▁▂▂▂▂▃▂▂▂▂▁▂▂▃▁▁▂▂▁▁▂▁▂▄▃▂▂▃▄</td></tr><tr><td>losses/explained_variance</td><td>▁▁▁▁▂▃▄▄▅▄▁▁▅▇▆▁▂▆▇▇▇▂▇▇▇▇▇▆▆▆▇▇▆▇▇█▄▆▇▆</td></tr><tr><td>losses/old_approx_kl</td><td>▄▅▄▄▆█▃▂▄▄▃▆▂▂▄▅▄▃▃▃▄▃▄▂▃▃▂▁▄▂▃▁▁▄▄▄▃▂▂▄</td></tr><tr><td>losses/policy_loss</td><td>▁█▂▃▄▄▃█▁▅▆▅▃▅▇▄█▃▇▃▇▅▆▆▅▆▆█▅▅▄▇▅▆▅▄▆▅▅▆</td></tr><tr><td>losses/value_loss</td><td>█▆▅▆▂▂▇▇▂▃▂▂▂▁▇▂▂▁▁▂▂▂▂▂▄▂▁▁▁▁▁▁▁▁▁▂▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>charts/SPS</td><td>1809</td></tr><tr><td>charts/episodic_length</td><td>1000</td></tr><tr><td>charts/episodic_return</td><td>129.51305</td></tr><tr><td>charts/learning_rate</td><td>1e-05</td></tr><tr><td>losses/approx_kl</td><td>1e-05</td></tr><tr><td>losses/clipfrac</td><td>0</td></tr><tr><td>losses/entropy</td><td>0.98343</td></tr><tr><td>losses/explained_variance</td><td>0.40983</td></tr><tr><td>losses/old_approx_kl</td><td>-6e-05</td></tr><tr><td>losses/policy_loss</td><td>-0.00017</td></tr><tr><td>losses/value_loss</td><td>44.41668</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Adversary-PPO-1748188977.5459409</strong> at: <a href='https://wandb.ai/thomasvroom-maastricht-university/RL/runs/bkglldwi' target=\"_blank\">https://wandb.ai/thomasvroom-maastricht-university/RL/runs/bkglldwi</a><br> View project at: <a href='https://wandb.ai/thomasvroom-maastricht-university/RL' target=\"_blank\">https://wandb.ai/thomasvroom-maastricht-university/RL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250525_180257-bkglldwi\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_name = f\"Adversary-PPO-{time.time()}\"\n",
    "train(config, run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eae6a8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected a total reward of: 211.13306806845753\n"
     ]
    }
   ],
   "source": [
    "# run_name = \"Adversary-DDQN-1748037372.4373443\"\n",
    "\n",
    "# load agent\n",
    "agent = PPO_Agent(8, 4).to(device)\n",
    "agent.load_state_dict(torch.load(f\"models/protagonist-{run_name}\"))\n",
    "class AgentWrapper:\n",
    "    def get_action(self, obs):\n",
    "        return agent.get_action_and_value(obs)[0]\n",
    "w_agent = AgentWrapper()\n",
    "\n",
    "util.visualize_episode(\n",
    "    env_id=\"CustomLunarLander-v0\",\n",
    "    gravity=config[\"gravity\"],\n",
    "    enable_wind=False,\n",
    "    wind_power=config[\"wind_power\"],\n",
    "    turbulence_power=config[\"turbulence_power\"],\n",
    "    agent=w_agent,\n",
    "    device=device,\n",
    "    max_time=30,\n",
    "    video_name=None\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
