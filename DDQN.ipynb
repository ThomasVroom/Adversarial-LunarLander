{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCGu9qGL6f3X"
      },
      "source": [
        "# Double Deep Q-Networks (DDQN)\n",
        "\n",
        "source: https://medium.com/@coldstart_coder/dqn-algorithm-training-an-ai-to-land-on-the-moon-1a1307748ed9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthomasvroom\u001b[0m (\u001b[33mthomasvroom-maastricht-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of cores available: 12\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import wandb\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import multiprocessing\n",
        "import gymnasium as gym\n",
        "from src.env import CustomLunarLander\n",
        "from src.models import DDQN_Agent, ReplayBuffer\n",
        "from src import util\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "wandb.login()\n",
        "np.seterr(all='raise'); # raise exceptions on errors\n",
        "print(f\"Number of cores available: {multiprocessing.cpu_count()}\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\") # device for pytorch\n",
        "gym.register(id=\"CustomLunarLander-v0\", entry_point=CustomLunarLander)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iJdIn2Fc2NnJ"
      },
      "outputs": [],
      "source": [
        "def train(config, run_name):\n",
        "    run = wandb.init(\n",
        "        project=\"RL\",\n",
        "        entity=\"thomasvroom-maastricht-university\",\n",
        "        config=config,\n",
        "        name=run_name\n",
        "    )\n",
        "\n",
        "    # seeding\n",
        "    random.seed(config[\"random_seed\"])\n",
        "    np.random.seed(config[\"random_seed\"])\n",
        "    torch.manual_seed(config[\"random_seed\"])\n",
        "    torch.backends.cudnn.deterministic = config[\"deterministic\"]\n",
        "\n",
        "    # create environment (only 1, since bottleneck isn't experience gathering)\n",
        "    env = gym.make(\n",
        "        id=\"CustomLunarLander-v0\",\n",
        "        gravity=config[\"gravity\"],\n",
        "        enable_wind=config[\"enable_wind\"],\n",
        "        wind_power=config[\"wind_power\"],\n",
        "        turbulence_power=config[\"turbulence_power\"],\n",
        "        max_episode_steps=config[\"max_env_steps\"]\n",
        "    )\n",
        "\n",
        "    agent = DDQN_Agent(env.observation_space.shape[0], env.action_space.n).to(device)\n",
        "    optimizer = torch.optim.AdamW(agent.parameters(), weight_decay=config[\"weight_decay\"], lr=config[\"learning_rate\"])\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    # create target network and replay buffer\n",
        "    target_network = type(agent)(env.observation_space.shape[0], env.action_space.n).to(device)\n",
        "    target_network.load_state_dict(agent.state_dict())\n",
        "    replay_buffer = ReplayBuffer(config[\"buffer_size\"])\n",
        "\n",
        "    epsilon = 1\n",
        "    learning_steps = 0\n",
        "\n",
        "    for episode in tqdm(range(config[\"train_episodes\"])):\n",
        "        # reset environment\n",
        "        state, _ = env.reset(seed=None if episode > 0 else config[\"random_seed\"])\n",
        "        done = False\n",
        "        truncated = False\n",
        "        total_reward = 0\n",
        "\n",
        "        # run environment until done\n",
        "        while not (done or truncated):\n",
        "            # epsilon-greedy action selection\n",
        "            if np.random.random() > epsilon:\n",
        "                with torch.no_grad():\n",
        "                    observation = torch.tensor(state, dtype=torch.float).to(device)\n",
        "                    action = agent.get_action(observation).item()\n",
        "            else:\n",
        "                action = env.action_space.sample()\n",
        "\n",
        "            # execute action\n",
        "            new_state, reward, done, truncated, _ = env.step(action)\n",
        "\n",
        "            # add sample to replay buffer\n",
        "            replay_buffer.add_new_sample(state, action, reward, new_state, done)\n",
        "\n",
        "            state = new_state\n",
        "            total_reward += reward\n",
        "\n",
        "            # only update weights if there are enough samples\n",
        "            if len(replay_buffer) > config[\"batch_size\"]:\n",
        "                # replace target network\n",
        "                if learning_steps % config[\"target_replace_steps\"] == 0:\n",
        "                    target_network.load_state_dict(agent.state_dict())\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # sample from replay buffer\n",
        "                samples = replay_buffer.get_pytorch_training_samples(device, config[\"batch_size\"])\n",
        "                states, actions, rewards, new_states, was_terminals = samples\n",
        "                indices = np.arange(config[\"batch_size\"])\n",
        "\n",
        "                # get the predicted q-values\n",
        "                q_pred = agent.forward(states)[indices, actions]\n",
        "\n",
        "                # get the estimated next q-values\n",
        "                q_next = target_network.forward(new_states).max(dim=1)[0]\n",
        "                q_next[was_terminals] = 0.0\n",
        "\n",
        "                # target values\n",
        "                q_label = rewards + config[\"gamma\"] * q_next\n",
        "\n",
        "                # calculate and backpropegate loss\n",
        "                loss = loss_fn(q_label, q_pred).to(device)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # decay epsilon\n",
        "                epsilon = max(epsilon - config[\"epsilon_decay\"], config[\"epsilon_min\"])\n",
        "\n",
        "                # record data\n",
        "                run.log({\"loss\": loss, \"epsilon\": epsilon}, learning_steps)\n",
        "                learning_steps += 1\n",
        "\n",
        "        run.log({\"total_reward\": total_reward}, max(learning_steps, episode))\n",
        "\n",
        "    env.close()\n",
        "    run.finish(0)\n",
        "    torch.save(agent.state_dict(), f\"models/{run_name}\")\n",
        "\n",
        "config = { # see: https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
        "    \"gravity\": -10.0,\n",
        "    \"wind_power\": 15.0,\n",
        "    \"turbulence_power\": 1.5,\n",
        "\n",
        "    \"random_seed\": 123,\n",
        "    \"deterministic\": True, # toggles torch.backends.cudnn.deterministic\n",
        "    \"train_episodes\": 2000,\n",
        "    \"buffer_size\": 100_000, # size of the replay buffer\n",
        "    \"batch_size\": 64,\n",
        "    \"target_replace_steps\": 500, # after how many steps the target network gets replaced\n",
        "    \"max_env_steps\": 1000, # number of steps before truncation\n",
        "\n",
        "    \"gamma\": 0.99,\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"epsilon_min\": 0.01,\n",
        "    \"epsilon_decay\": 5e-6\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training without wind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config[\"enable_wind\"] = False\n",
        "run_name = f\"DDQN-NoWind-{time.time()}\"\n",
        "train(config, run_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training with wind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">DDQN-Wind-1748101329.4314895</strong> at: <a href='https://wandb.ai/thomasvroom-maastricht-university/RL/runs/jq3ij54m' target=\"_blank\">https://wandb.ai/thomasvroom-maastricht-university/RL/runs/jq3ij54m</a><br> View project at: <a href='https://wandb.ai/thomasvroom-maastricht-university/RL' target=\"_blank\">https://wandb.ai/thomasvroom-maastricht-university/RL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250524_174209-jq3ij54m\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Thoma\\OneDrive\\Documenten\\University\\Year 4\\Period 5\\Reinforcement Learning\\Assignment\\wandb\\run-20250524_174241-al59y6qp</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/thomasvroom-maastricht-university/RL/runs/al59y6qp' target=\"_blank\">DDQN-Wind-1748101361.733588</a></strong> to <a href='https://wandb.ai/thomasvroom-maastricht-university/RL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/thomasvroom-maastricht-university/RL' target=\"_blank\">https://wandb.ai/thomasvroom-maastricht-university/RL</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/thomasvroom-maastricht-university/RL/runs/al59y6qp' target=\"_blank\">https://wandb.ai/thomasvroom-maastricht-university/RL/runs/al59y6qp</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [38:07<00:00,  1.14s/it]\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epsilon</td><td>██▆▆▆▅▅▅▅▃▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>▇▆█▂▅▂▂▇▁▁▁▁▁▁▅▁▁█▂▁▂▄▂▁▁▁▁▂▂▁▃▁▁▂▁▁▁▃▁▁</td></tr><tr><td>total_reward</td><td>▄▄▂▃▄▂▁▄▂▂▁▂▄▃▃▄▄▃▂▄▄▂▃▃▄▃▃▃▇▅▇▇▇▅▄█▇██▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epsilon</td><td>0.01</td></tr><tr><td>loss</td><td>15.41412</td></tr><tr><td>total_reward</td><td>248.45203</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">DDQN-Wind-1748101361.733588</strong> at: <a href='https://wandb.ai/thomasvroom-maastricht-university/RL/runs/al59y6qp' target=\"_blank\">https://wandb.ai/thomasvroom-maastricht-university/RL/runs/al59y6qp</a><br> View project at: <a href='https://wandb.ai/thomasvroom-maastricht-university/RL' target=\"_blank\">https://wandb.ai/thomasvroom-maastricht-university/RL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250524_174241-al59y6qp\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "config[\"enable_wind\"] = True\n",
        "run_name = f\"DDQN-Wind-{time.time()}\"\n",
        "train(config, run_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize Episode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collected a total reward of: 255.88928824183392\n"
          ]
        }
      ],
      "source": [
        "# run_name = \n",
        "\n",
        "# load agent\n",
        "agent = DDQN_Agent(8, 4).to(device)\n",
        "agent.load_state_dict(torch.load(f\"models/{run_name}\"))\n",
        "\n",
        "util.visualize_episode(\n",
        "    env_id=\"CustomLunarLander-v0\",\n",
        "    gravity=config[\"gravity\"],\n",
        "    enable_wind=False,\n",
        "    wind_power=config[\"wind_power\"],\n",
        "    turbulence_power=config[\"turbulence_power\"],\n",
        "    agent=agent,\n",
        "    device=device,\n",
        "    max_time=30,\n",
        "    video_name=None\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
