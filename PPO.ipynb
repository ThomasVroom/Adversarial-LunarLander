{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "207281d5",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization (PPO)\n",
    "\n",
    "source: https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24dd4d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthomasvroom\u001b[0m (\u001b[33mthomasvroom-maastricht-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cores available: 12\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import wandb\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "import gymnasium as gym\n",
    "\n",
    "from src.env import CustomLunarLander\n",
    "from src.models import PPO_Agent\n",
    "from src import util\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "wandb.login()\n",
    "np.seterr(all='raise'); # raise exceptions on errors\n",
    "print(f\"Number of cores available: {multiprocessing.cpu_count()}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\") # device for pytorch\n",
    "gym.register(id=\"CustomLunarLander-v0\", entry_point=CustomLunarLander)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9cbf110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, run_name):\n",
    "    run = wandb.init(\n",
    "        project=\"RL\",\n",
    "        entity=\"thomasvroom-maastricht-university\",\n",
    "        config=config,\n",
    "        name=run_name\n",
    "    )\n",
    "\n",
    "    # seeding\n",
    "    random.seed(config[\"random_seed\"])\n",
    "    np.random.seed(config[\"random_seed\"])\n",
    "    torch.manual_seed(config[\"random_seed\"])\n",
    "    torch.backends.cudnn.deterministic = config[\"deterministic\"]\n",
    "\n",
    "    # create environments\n",
    "    envs = gym.vector.SyncVectorEnv([\n",
    "        util.make_env(\n",
    "            env_id=\"CustomLunarLander-v0\",\n",
    "            gravity=config[\"gravity\"],\n",
    "            enable_wind=config[\"enable_wind\"],\n",
    "            wind_power=config[\"wind_power\"],\n",
    "            turbulence_power=config[\"turbulence_power\"],\n",
    "            max_episode_steps=config[\"max_env_steps\"]\n",
    "        ) for _ in range(config[\"n_envs\"])],\n",
    "    )\n",
    "\n",
    "    agent = PPO_Agent(np.array(envs.single_observation_space.shape).prod(), envs.single_action_space.n).to(device)\n",
    "    optimizer = torch.optim.Adam(agent.parameters(), lr=config[\"learning_rate\"], eps=1e-5)\n",
    "\n",
    "    # more hyperparameters determined at runtime\n",
    "    batch_size = int(config[\"n_envs\"] * config[\"steps_per_batch\"])\n",
    "    minibatch_size = int(batch_size // config[\"num_minibatches\"])\n",
    "    num_iterations = int(config[\"train_steps\"] // batch_size)\n",
    "\n",
    "    # storage setup\n",
    "    obs = torch.zeros((config[\"steps_per_batch\"], config[\"n_envs\"]) + envs.single_observation_space.shape).to(device)\n",
    "    actions = torch.zeros((config[\"steps_per_batch\"], config[\"n_envs\"]) + envs.single_action_space.shape).to(device)\n",
    "    logprobs = torch.zeros((config[\"steps_per_batch\"], config[\"n_envs\"])).to(device)\n",
    "    rewards = torch.zeros((config[\"steps_per_batch\"], config[\"n_envs\"])).to(device)\n",
    "    dones = torch.zeros((config[\"steps_per_batch\"], config[\"n_envs\"])).to(device)\n",
    "    values = torch.zeros((config[\"steps_per_batch\"], config[\"n_envs\"])).to(device)\n",
    "\n",
    "    # start the environment\n",
    "    global_step = 0\n",
    "    start_time = time.time()\n",
    "    next_obs, _ = envs.reset(seed=config[\"random_seed\"])\n",
    "    next_obs = torch.Tensor(next_obs).to(device)\n",
    "    next_done = torch.zeros(config[\"n_envs\"]).to(device)\n",
    "\n",
    "    for iteration in tqdm(range(1, num_iterations + 1)):\n",
    "        # annealing the learning rate\n",
    "        if config[\"anneal_lr\"]:\n",
    "            frac = 1.0 - (iteration - 1.0) / num_iterations\n",
    "            lrnow = frac * config[\"learning_rate\"]\n",
    "            optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "        for step in range(0, config[\"steps_per_batch\"]):\n",
    "            global_step += config[\"n_envs\"]\n",
    "            obs[step] = next_obs\n",
    "            dones[step] = next_done\n",
    "\n",
    "            # action logic\n",
    "            with torch.no_grad():\n",
    "                action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
    "                values[step] = value.flatten()\n",
    "            actions[step] = action\n",
    "            logprobs[step] = logprob\n",
    "\n",
    "            # execute the action and log data\n",
    "            next_obs, reward, terminations, truncations, infos = envs.step(action.cpu().numpy())\n",
    "            next_done = np.logical_or(terminations, truncations)\n",
    "            rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)\n",
    "\n",
    "            if infos and \"episode\" in infos:\n",
    "                indices = np.where(infos[\"_episode\"])\n",
    "                run.log({\n",
    "                    \"charts/episodic_return\": infos[\"episode\"][\"r\"][indices].mean(),\n",
    "                    \"charts/episodic_length\": infos[\"episode\"][\"l\"][indices].mean()\n",
    "                }, global_step)\n",
    "\n",
    "        # bootstrap value if not done\n",
    "        with torch.no_grad():\n",
    "            next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "            advantages = torch.zeros_like(rewards).to(device)\n",
    "            lastgaelam = 0\n",
    "            for t in reversed(range(config[\"steps_per_batch\"])):\n",
    "                if t == config[\"steps_per_batch\"] - 1:\n",
    "                    nextnonterminal = 1.0 - next_done\n",
    "                    nextvalues = next_value\n",
    "                else:\n",
    "                    nextnonterminal = 1.0 - dones[t + 1]\n",
    "                    nextvalues = values[t + 1]\n",
    "                delta = rewards[t] + config[\"gamma\"] * nextvalues * nextnonterminal - values[t]\n",
    "                advantages[t] = lastgaelam = delta + config[\"gamma\"] * config[\"gae_lambda\"] * nextnonterminal * lastgaelam\n",
    "            returns = advantages + values\n",
    "\n",
    "        # flatten the batch\n",
    "        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "        b_logprobs = logprobs.reshape(-1)\n",
    "        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "        b_advantages = advantages.reshape(-1)\n",
    "        b_returns = returns.reshape(-1)\n",
    "        b_values = values.reshape(-1)\n",
    "\n",
    "        # optimizing the policy and value network\n",
    "        b_inds = np.arange(batch_size)\n",
    "        clipfracs = []\n",
    "        for epoch in range(config[\"policy_epochs\"]):\n",
    "            np.random.shuffle(b_inds)\n",
    "            for start in range(0, batch_size, minibatch_size):\n",
    "                end = start + minibatch_size\n",
    "                mb_inds = b_inds[start:end]\n",
    "\n",
    "                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])\n",
    "                logratio = newlogprob - b_logprobs[mb_inds]\n",
    "                ratio = logratio.exp()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                    old_approx_kl = (-logratio).mean()\n",
    "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                    clipfracs += [((ratio - 1.0).abs() > config[\"clip_coef\"]).float().mean().item()]\n",
    "\n",
    "                mb_advantages = b_advantages[mb_inds]\n",
    "                if config[\"norm_adv\"]:\n",
    "                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "                # policy loss\n",
    "                pg_loss1 = -mb_advantages * ratio\n",
    "                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - config[\"clip_coef\"], 1 + config[\"clip_coef\"])\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                # value loss\n",
    "                newvalue = newvalue.view(-1)\n",
    "                if config[\"clip_vloss\"]:\n",
    "                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                    v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                        newvalue - b_values[mb_inds],\n",
    "                        -config[\"clip_coef\"],\n",
    "                        config[\"clip_coef\"],\n",
    "                    )\n",
    "                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                    v_loss = 0.5 * v_loss_max.mean()\n",
    "                else:\n",
    "                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "                entropy_loss = entropy.mean()\n",
    "                loss = pg_loss - config[\"ent_coef\"] * entropy_loss + v_loss * config[\"vf_coef\"]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(agent.parameters(), config[\"max_grad_norm\"])\n",
    "                optimizer.step()\n",
    "\n",
    "            if config[\"target_kl\"] is not None and approx_kl > config[\"target_kl\"]:\n",
    "                break\n",
    "\n",
    "        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "        # record data\n",
    "        run.log({\n",
    "            \"charts/learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
    "            \"losses/value_loss\": v_loss.item(),\n",
    "            \"losses/policy_loss\": pg_loss.item(),\n",
    "            \"losses/entropy\": entropy_loss.item(),\n",
    "            \"losses/old_approx_kl\": old_approx_kl.item(),\n",
    "            \"losses/approx_kl\": approx_kl.item(),\n",
    "            \"losses/clipfrac\": np.mean(clipfracs),\n",
    "            \"charts/SPS\": int(global_step / (time.time() - start_time))\n",
    "        }, global_step)\n",
    "        if not np.isnan(explained_var):\n",
    "            run.log({\"losses/explained_variance\": explained_var}, global_step)\n",
    "\n",
    "    envs.close()\n",
    "    run.finish(0)\n",
    "    torch.save(agent.state_dict(), f\"models/{run_name}\")\n",
    "\n",
    "config = { # see: https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
    "    \"gravity\": -10.0,\n",
    "    \"wind_power\": 15.0,\n",
    "    \"turbulence_power\": 1.5,\n",
    "\n",
    "    \"random_seed\": 123,\n",
    "    \"deterministic\": True, # toggles torch.backends.cudnn.deterministic\n",
    "    \"n_envs\": 8,\n",
    "    \"train_steps\": 4_000_000,\n",
    "    \"steps_per_batch\": 2048, # number of steps to run in each env per policy rollout\n",
    "    \"num_minibatches\": 16,\n",
    "    \"policy_epochs\": 8, # number of epochs to update the policy\n",
    "    \"max_env_steps\": 1000, # number of steps before truncation\n",
    "\n",
    "    \"gamma\": 0.99,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"anneal_lr\": True, # toggles lr decay\n",
    "    \"gae_lambda\": 0.95, # lambda for the general advantage estimation\n",
    "    \"clip_coef\": 0.2, # surrogate clipping coefficient\n",
    "    \"norm_adv\": True, # toggles advantage normalization\n",
    "    \"clip_vloss\": False, # toggles use of clipped loss for the value function\n",
    "    \"ent_coef\": 0.01, # entropy coefficient\n",
    "    \"vf_coef\": 0.5, # value function coefficient\n",
    "    \"max_grad_norm\": 0.5, # maximum norm for gradient clipping\n",
    "    \"target_kl\": 0.01 # the target KL divergence threshold\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f370f06",
   "metadata": {},
   "source": [
    "### Training without wind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33da7801",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"enable_wind\"] = False\n",
    "run_name = f\"PPO-NoWind-{time.time()}\"\n",
    "train(config, run_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90d3725",
   "metadata": {},
   "source": [
    "### Training with wind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83a15d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Thoma\\OneDrive\\Documenten\\University\\Year 4\\Period 5\\Reinforcement Learning\\Assignment\\wandb\\run-20250523_223138-0wnnseur</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thomasvroom-maastricht-university/RL/runs/0wnnseur' target=\"_blank\">PPO-Wind-1748032298.1644526</a></strong> to <a href='https://wandb.ai/thomasvroom-maastricht-university/RL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thomasvroom-maastricht-university/RL' target=\"_blank\">https://wandb.ai/thomasvroom-maastricht-university/RL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thomasvroom-maastricht-university/RL/runs/0wnnseur' target=\"_blank\">https://wandb.ai/thomasvroom-maastricht-university/RL/runs/0wnnseur</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 244/244 [26:08<00:00,  6.43s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>charts/SPS</td><td>▁▄▆▇▇▇▇▇▇█▇█████████████████████████████</td></tr><tr><td>charts/episodic_length</td><td>▁▁▁▁▁▁▂▁▁▁▂▁▂▂█▃▄▃▄▄▂▂█▄▃▂▂▆▁▃▆▃▂▂▂▂▂███</td></tr><tr><td>charts/episodic_return</td><td>▂▁▄▄▃▄▄▄▃▄▆▅▄▄▄▄▄▇▇█▇▄▇▇▇▇▃▆█▇▇█▇▇▇█▇▇██</td></tr><tr><td>charts/learning_rate</td><td>██▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>losses/approx_kl</td><td>▆█▄▅▇▅▅▃▃▄▄▄▄▄▄▃▃▃▄▄▄▅▄▄▃▅▃▄▃▃▂▄▃▄▃▁▃▄▂▁</td></tr><tr><td>losses/clipfrac</td><td>▁▅▄▅▃█▅▂▄▃▂▄▃▂▃▁▂▂▃▂▃▃▂▄▂▂▁▂▂▂▂▄▂▁▂▃▁▁▁▁</td></tr><tr><td>losses/entropy</td><td>██▇▇▆▅▅▅▄▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▂▁▁▁▁</td></tr><tr><td>losses/explained_variance</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▃▄▄▄▄▅▄▆▆▆▇▇▇▆▇▇▇██▇▇▇▇▇██▇▇</td></tr><tr><td>losses/old_approx_kl</td><td>▁█▆▅▄▅▃▃▅▂▂▆▄▄▆▅▂▄▄▂▅▂▂▂▄▃▄▂▃▅▂▄▃▁▂▁▃▂▁▂</td></tr><tr><td>losses/policy_loss</td><td>▄▃▅▁▅█▃▄▆▄▅▆▄▆▆▃▅▅▆▄▅▆▅▅▅▆▅▆▅▆▆▆▅▆▄▄▄▆▆▆</td></tr><tr><td>losses/value_loss</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>charts/SPS</td><td>2548</td></tr><tr><td>charts/episodic_length</td><td>236</td></tr><tr><td>charts/episodic_return</td><td>260.47493</td></tr><tr><td>charts/learning_rate</td><td>0.0</td></tr><tr><td>losses/approx_kl</td><td>0.0</td></tr><tr><td>losses/clipfrac</td><td>0</td></tr><tr><td>losses/entropy</td><td>0.49484</td></tr><tr><td>losses/explained_variance</td><td>0.71619</td></tr><tr><td>losses/old_approx_kl</td><td>2e-05</td></tr><tr><td>losses/policy_loss</td><td>-6e-05</td></tr><tr><td>losses/value_loss</td><td>129.10643</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">PPO-Wind-1748032298.1644526</strong> at: <a href='https://wandb.ai/thomasvroom-maastricht-university/RL/runs/0wnnseur' target=\"_blank\">https://wandb.ai/thomasvroom-maastricht-university/RL/runs/0wnnseur</a><br> View project at: <a href='https://wandb.ai/thomasvroom-maastricht-university/RL' target=\"_blank\">https://wandb.ai/thomasvroom-maastricht-university/RL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250523_223138-0wnnseur\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config[\"enable_wind\"] = True\n",
    "run_name = f\"PPO-Wind-{time.time()}\"\n",
    "train(config, run_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14c1046",
   "metadata": {},
   "source": [
    "### Visualize Episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5bfd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_name = \n",
    "\n",
    "# load agent\n",
    "agent = PPO_Agent(8, 4).to(device)\n",
    "agent.load_state_dict(torch.load(f\"models/{run_name}\"))\n",
    "class AgentWrapper:\n",
    "    def get_action(self, obs):\n",
    "        return agent.get_action_and_value(obs)[0]\n",
    "w_agent = AgentWrapper()\n",
    "\n",
    "util.visualize_episode(\n",
    "    env_id=\"CustomLunarLander-v0\",\n",
    "    gravity=config[\"gravity\"],\n",
    "    enable_wind=False,\n",
    "    wind_power=config[\"wind_power\"],\n",
    "    turbulence_power=config[\"turbulence_power\"],\n",
    "    agent=w_agent,\n",
    "    device=device,\n",
    "    max_time=30,\n",
    "    video_name=None\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
